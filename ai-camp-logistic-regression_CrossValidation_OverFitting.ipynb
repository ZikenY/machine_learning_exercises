{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c21ce616a925aa879e2d55dc7ab9921ab4fdbca0"
   },
   "source": [
    "## Logistic Regression模型优化，过拟合实验，梯度检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "c4ffb0df509bded90e744c895d8d1653345de5da"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "5d1243ea6129fa04af13e5e67d7609d31b4a25c4"
   },
   "outputs": [],
   "source": [
    "# read data using pandas\n",
    "data = pd.read_csv(\"./HR_comma_sep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "29b2f88b7f802e0be0ea97c3894ab1b1e9ce32b0"
   },
   "outputs": [],
   "source": [
    "# all samples, lables in X, y\n",
    "y, X = dmatrices('left~satisfaction_level+last_evaluation+number_project+average_montly_hours+time_spend_company+Work_accident+promotion_last_5years+C(sales)+C(salary)', data, return_type='dataframe')\n",
    "X = np.asmatrix(X)\n",
    "y = np.ravel(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a1a1ff1ce7b1027a211d97554480b7b453fa45b"
   },
   "source": [
    "将所有列的值归一化到[0,1]区间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "d14bac22149ba3718a780c0eb8e88065bb592300"
   },
   "outputs": [],
   "source": [
    "# normalize all feature values into [0, 1]\n",
    "# X.shape[1]: feature count\n",
    "for i in range(1, X.shape[1]):\n",
    "    xmin = X[:,i].min() # get minimal value from all row for column i(feature i)\n",
    "    xmax = X[:,i].max()\n",
    "    X[:, i] = (X[:, i] - xmin) / (xmax - xmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手写logistic regression，gradient descent 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "59198c3aabae5b62830bafca7fc7dd87d8eac529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=0 loss=1.1203823278066718 error=0.5037002466831122\n",
      "T=5 loss=0.6492666637968592 error=0.2910194012934196\n",
      "T=10 loss=0.6095807663133694 error=0.26668444562970867\n",
      "T=15 loss=0.5816449211566243 error=0.25888392559503964\n",
      "Gradient Descent done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nT=0 loss=1.1203823278066718 error=0.5037002466831122\\nT=5 loss=0.6492666637968592 error=0.2910194012934196\\nT=10 loss=0.6095807663133694 error=0.26668444562970867\\nT=15 loss=0.5816449211566243 error=0.25888392559503964\\n            ......\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "alpha = 1  # learning rate\n",
    "beta = np.random.randn(X.shape[1]) # initial β's all components to normal distribution\n",
    "iteration_count = 200\n",
    "losses = list()  # collect loss for each iteration\n",
    "errors = list()  # collect error rate for each iteration\n",
    "\n",
    "for T in range(iteration_count):\n",
    "    # prob = 1 / (1 + exp(-β0*X0 - β1*X1 - β2*X2...... - βn*Xn))   \n",
    "    #                             matrix_X：n*m； vector β：size=m\n",
    "    # the following \"1.\" will be expanded to m-Dimension automatically   \n",
    "    prob = np.array(1. / (1 + np.exp(-np.matmul(X, beta)))) # prob is the \"scoring function\"\n",
    "#    print('prob.shape: ', prob.shape) # (1, 14999)\n",
    "    prob = prob.ravel()\n",
    "#    print('prob.shape(after .ravel()): ', prob.shape) # (14999,)    \n",
    "\n",
    "    # zip each \"sample -> score\" pair\n",
    "    prob_y = list(zip(prob, y))\n",
    "#    print('prob_y: ', prob_y)  # [(0.8563635682985654, 1.0), (0.11839265197520915, 1.0), .......]\n",
    "    \n",
    "    # compute cross_entropy loss. (along each dimension)\n",
    "    loss = -sum([np.log(p) if y == 1 else np.log(1 - p) for p, y in prob_y]) / len(y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # computer error_rate(only for observation)\n",
    "    # divide_line = 0.5    \n",
    "    error_rate = 0\n",
    "    for i in range(len(y)):\n",
    "        if ((prob[i] > 0.5 and y[i] == 0) or (prob[i] <= 0.5 and y[i] == 1)):\n",
    "            error_rate += 1;\n",
    "    error_rate /= len(y) # wrong_y : all y\n",
    "    errors.append(error_rate)\n",
    "    \n",
    "    if T % 5 ==0 :\n",
    "        print('T=' + str(T) + ' loss=' + str(loss) + ' error=' + str(error_rate))\n",
    "    \n",
    "    # 计算损失函数关于beta每个分量的导数\n",
    "    # compute the partial derivative in terms of each component of beta.\n",
    "    # X.shape[1] is feature count. we should compute such number of partial derivatives parallelly to each sample\n",
    "    deriv = np.zeros(X.shape[1])\n",
    "#    print('deriv.shape: ', deriv.shape)   # (19,) such number of β's derivatives\n",
    "    \n",
    "    # computer each component of β's partial derivative of all samples\n",
    "    for i in range(len(y)):\n",
    "        # extract the whole line (features) of sample_i\n",
    "        Xi = np.asarray(X[i,:]).ravel()\n",
    "        # Xi is a vector\n",
    "        deriv += Xi * (prob[i] - y[i]) \n",
    "        \n",
    "    # get average of derivatives of sample/labels\n",
    "    deriv /= len(y)\n",
    "    \n",
    "    # 沿导数相反方向修改beta\n",
    "    # descent the gradient of β (in all dimensions)\n",
    "    beta -= alpha * deriv\n",
    "\n",
    "print('Gradient Descent done.')\n",
    "'''\n",
    "T=0 loss=1.1203823278066718 error=0.5037002466831122\n",
    "T=5 loss=0.6492666637968592 error=0.2910194012934196\n",
    "T=10 loss=0.6095807663133694 error=0.26668444562970867\n",
    "T=15 loss=0.5816449211566243 error=0.25888392559503964\n",
    "            ......\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split X, y into training_set and validation_set as 7:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sk-learn dataset split, cross validation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "\n",
    "# training_set/testing_set : 7:3\n",
    "Xtrain, Xvali, ytrain, yvali = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.7926666666666666\n",
      "confusion matrix: \n",
      " [[3208  254]\n",
      " [ 679  359]]\n"
     ]
    }
   ],
   "source": [
    "# 生成各项测试指标\n",
    "from sklearn import metrics\n",
    "pred = model_LR.predict(Xvali)\n",
    "print('accuracy score: ', metrics.accuracy_score(yvali, pred))\n",
    "print('confusion matrix: \\n', metrics.confusion_matrix(yvali, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10份交叉验证\n",
    "### Cross Validation for cv = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.7861785463015761  for C =  10.0\n"
     ]
    }
   ],
   "source": [
    "# cross_validation (mean of 10 times):\n",
    "C = 1e1\n",
    "accuracy_score = cross_val_score(LogisticRegression(C = C, penalty = 'l2'), X, y, scoring = 'accuracy', cv = 10).mean()\n",
    "print('accuracy_score: ', accuracy_score, ' for C = ', C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.7861118796349095  for C =  1000.0\n"
     ]
    }
   ],
   "source": [
    "C = 1e3\n",
    "accuracy_score = cross_val_score(LogisticRegression(C = C), X, y, scoring = 'accuracy', cv = 10).mean()\n",
    "print('accuracy_score: ', accuracy_score, ' for C = ', C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.7849117017385341  for C =  1.0\n"
     ]
    }
   ],
   "source": [
    "C = 1e0\n",
    "accuracy_score = cross_val_score(LogisticRegression(C = C), X, y, scoring = 'accuracy', cv = 10).mean()\n",
    "print('accuracy_score: ', accuracy_score, ' for C = ', C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.7619174793411018  for C =  0.001\n"
     ]
    }
   ],
   "source": [
    "C = 0.001\n",
    "accuracy_score = cross_val_score(LogisticRegression(C = C), X, y, scoring = 'accuracy', cv = 10).mean()\n",
    "print('accuracy_score: ', accuracy_score, ' for C = ', C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, C = 1e3 has choosen. fit it on all data!!!\n",
    "model_LR = LogisticRegression(C=1e3)\n",
    "model_LR.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "053c68f27b5478aabe20e91d1a15c539569a7235"
   },
   "source": [
    "## 过拟合实验\n",
    "### overfitting test for error_rate comparsion between training_set and validation_set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "e7e16809b6b7e8a64c8aa2f725bf7c28d1c73b2d"
   },
   "outputs": [],
   "source": [
    "# training_set/testing_set : 7:3\n",
    "Xtrain, Xvali, ytrain, yvali = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "408462cdfcb4c58311172cb2831e33d2311b3ea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=0 loss=1.135137140973581 error=0.5020478140775312\n",
      "T=5 loss=0.6444185582182453 error=0.29631393466044387\n",
      "T=10 loss=0.6030253387203952 error=0.27107343556529195\n",
      "T=15 loss=0.575467884391503 error=0.2631679207543576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nT=0 loss=1.135137140973581 error=0.5020478140775312\\nT=5 loss=0.6444185582182453 error=0.29631393466044387\\nT=10 loss=0.6030253387203952 error=0.27107343556529195\\nT=15 loss=0.575467884391503 error=0.2631679207543576\\n        ......\\nT=200\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "alpha = 1  # learning rate\n",
    "beta = np.random.randn(Xtrain.shape[1]) # 随机初始化参数beta\n",
    "error_rates_train=[]\n",
    "error_rates_vali=[]\n",
    "iteration_count = 200\n",
    "for T in range(iteration_count):\n",
    "    prob = np.array(1. / (1 + np.exp(-np.matmul(Xtrain, beta)))).ravel()  # 根据当前beta预测离职的概率\n",
    "    prob_y = list(zip(prob, ytrain))\n",
    "    loss = -sum([np.log(p) if y == 1 else np.log(1 - p) for p, y in prob_y]) / len(ytrain) # 计算损失函数的值\n",
    "    error_rate = 0\n",
    "    for i in range(len(ytrain)):\n",
    "        if ((prob[i] > 0.5 and ytrain[i] == 0) or (prob[i] <= 0.5 and ytrain[i] == 1)):\n",
    "            error_rate += 1;\n",
    "    error_rate /= len(ytrain)\n",
    "    error_rates_train.append(error_rate)\n",
    "    \n",
    "    prob_vali = np.array(1. / (1 + np.exp(-np.matmul(Xvali, beta)))).ravel()  # 根据当前beta预测离职的概率\n",
    "    prob_y_vali = list(zip(prob_vali, yvali))\n",
    "    loss = -sum([np.log(p) if y == 1 else np.log(1 - p) for p, y in prob_y_vali]) / len(yvali) # 计算损失函数的值\n",
    "    error_rate_vali = 0\n",
    "    for i in range(len(yvali)):\n",
    "        if ((prob[i] > 0.5 and yvali[i] == 0) or (prob[i] <= 0.5 and yvali[i] == 1)):\n",
    "            error_rate_vali += 1\n",
    "    error_rate_vali /= len(yvali)\n",
    "    error_rates_vali.append(error_rate_vali)\n",
    "    \n",
    "    if T % 5 ==0 :\n",
    "        print('T=' + str(T) + ' loss=' + str(loss) + ' error=' + str(error_rate))\n",
    "    # 计算损失函数关于beta每个分量的导数\n",
    "    deriv = np.zeros(Xtrain.shape[1])\n",
    "    for i in range(len(ytrain)):\n",
    "        deriv += np.asarray(Xtrain[i,:]).ravel() * (prob[i] - ytrain[i])\n",
    "    deriv /= len(ytrain)\n",
    "    # 沿导数相反方向修改beta\n",
    "    beta -= alpha * deriv\n",
    "'''\n",
    "T=0 loss=1.135137140973581 error=0.5020478140775312\n",
    "T=5 loss=0.6444185582182453 error=0.29631393466044387\n",
    "T=10 loss=0.6030253387203952 error=0.27107343556529195\n",
    "T=15 loss=0.575467884391503 error=0.2631679207543576\n",
    "        ......\n",
    "T=200\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "9cc145a597508833998efac5d82b265c145cafe5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0,200), error_rates_train, 'r^', range(0, 200), error_rates_vali, 'bs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "726100cbf916ee8812d429ccfd45d21c3b91632b"
   },
   "source": [
    "# 梯度检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "e44c12545b057f5ffca368f9e5447b4aa9a3f1f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We calculated 0.3306924011046167\n",
      "According to definition of gradient, it is 0.33069903233373665\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "alpha = 1  # learning rate\n",
    "beta = np.random.randn(X.shape[1]) # 随机初始化参数beta\n",
    "\n",
    "#dF/dbeta0\n",
    "prob = np.array(1. / (1 + np.exp(-np.matmul(X, beta)))).ravel()  # 根据当前beta预测离职的概率\n",
    "prob_y = list(zip(prob, y))\n",
    "loss = -sum([np.log(p) if y == 1 else np.log(1. - p) for p, y in prob_y]) / len(y) # 计算损失函数的值\n",
    "deriv = np.zeros(X.shape[1])\n",
    "for i in range(len(y)):\n",
    "    deriv += np.asarray(X[i,:]).ravel() * (prob[i] - y[i])\n",
    "deriv /= len(y)\n",
    "print('We calculated ' + str(deriv[0]))\n",
    "\n",
    "delta = 0.0001\n",
    "beta[0] += delta\n",
    "prob = np.array(1. / (1 + np.exp(-np.matmul(X, beta)))).ravel()  # 根据当前beta预测离职的概率\n",
    "prob_y = list(zip(prob, y))\n",
    "loss2 = -sum([np.log(p) if y == 1 else np.log(1. - p) for p, y in prob_y]) / len(y) # 计算损失函数的值\n",
    "shouldbe = (loss2 - loss) / delta # (F(b0+delta,b1,...,bn) - F(b0,...bn)) / delta\n",
    "print('According to definition of gradient, it is ' + str(shouldbe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d1a7db52accdeb216085df337cd05e2119e3c71",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
